{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8319923,"sourceType":"datasetVersion","datasetId":4941900}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-18T10:50:57.791796Z","iopub.execute_input":"2024-05-18T10:50:57.792831Z","iopub.status.idle":"2024-05-18T10:50:58.706886Z","shell.execute_reply.started":"2024-05-18T10:50:57.792798Z","shell.execute_reply":"2024-05-18T10:50:58.705943Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/vqa-dataset/vqa_dataset/data-00016-of-00034.arrow\n/kaggle/input/vqa-dataset/vqa_dataset/data-00004-of-00034.arrow\n/kaggle/input/vqa-dataset/vqa_dataset/state.json\n/kaggle/input/vqa-dataset/vqa_dataset/data-00011-of-00034.arrow\n/kaggle/input/vqa-dataset/vqa_dataset/dataset_info.json\n/kaggle/input/vqa-dataset/vqa_dataset/data-00002-of-00034.arrow\n/kaggle/input/vqa-dataset/vqa_dataset/data-00028-of-00034.arrow\n/kaggle/input/vqa-dataset/vqa_dataset/data-00015-of-00034.arrow\n/kaggle/input/vqa-dataset/vqa_dataset/data-00013-of-00034.arrow\n/kaggle/input/vqa-dataset/vqa_dataset/data-00019-of-00034.arrow\n/kaggle/input/vqa-dataset/vqa_dataset/data-00027-of-00034.arrow\n/kaggle/input/vqa-dataset/vqa_dataset/data-00031-of-00034.arrow\n/kaggle/input/vqa-dataset/vqa_dataset/data-00006-of-00034.arrow\n/kaggle/input/vqa-dataset/vqa_dataset/data-00033-of-00034.arrow\n/kaggle/input/vqa-dataset/vqa_dataset/data-00008-of-00034.arrow\n/kaggle/input/vqa-dataset/vqa_dataset/data-00025-of-00034.arrow\n/kaggle/input/vqa-dataset/vqa_dataset/data-00005-of-00034.arrow\n/kaggle/input/vqa-dataset/vqa_dataset/data-00003-of-00034.arrow\n/kaggle/input/vqa-dataset/vqa_dataset/data-00032-of-00034.arrow\n/kaggle/input/vqa-dataset/vqa_dataset/data-00022-of-00034.arrow\n/kaggle/input/vqa-dataset/vqa_dataset/data-00018-of-00034.arrow\n/kaggle/input/vqa-dataset/vqa_dataset/data-00007-of-00034.arrow\n/kaggle/input/vqa-dataset/vqa_dataset/data-00012-of-00034.arrow\n/kaggle/input/vqa-dataset/vqa_dataset/data-00029-of-00034.arrow\n/kaggle/input/vqa-dataset/vqa_dataset/data-00024-of-00034.arrow\n/kaggle/input/vqa-dataset/vqa_dataset/data-00021-of-00034.arrow\n/kaggle/input/vqa-dataset/vqa_dataset/data-00023-of-00034.arrow\n/kaggle/input/vqa-dataset/vqa_dataset/data-00000-of-00034.arrow\n/kaggle/input/vqa-dataset/vqa_dataset/data-00030-of-00034.arrow\n/kaggle/input/vqa-dataset/vqa_dataset/data-00017-of-00034.arrow\n/kaggle/input/vqa-dataset/vqa_dataset/data-00020-of-00034.arrow\n/kaggle/input/vqa-dataset/vqa_dataset/data-00010-of-00034.arrow\n/kaggle/input/vqa-dataset/vqa_dataset/data-00001-of-00034.arrow\n/kaggle/input/vqa-dataset/vqa_dataset/data-00009-of-00034.arrow\n/kaggle/input/vqa-dataset/vqa_dataset/data-00026-of-00034.arrow\n/kaggle/input/vqa-dataset/vqa_dataset/data-00014-of-00034.arrow\n","output_type":"stream"}]},{"cell_type":"code","source":"from datasets import load_from_disk\ndataset = load_from_disk(\"/kaggle/input/vqa-dataset/vqa_dataset\")","metadata":{"execution":{"iopub.status.busy":"2024-05-18T10:50:58.708655Z","iopub.execute_input":"2024-05-18T10:50:58.709035Z","iopub.status.idle":"2024-05-18T10:50:59.936185Z","shell.execute_reply.started":"2024-05-18T10:50:58.709010Z","shell.execute_reply":"2024-05-18T10:50:59.935163Z"},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading dataset from disk:   0%|          | 0/34 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea74a14f8bbb4d6fafa3cbaa68c8feda"}},"metadata":{}}]},{"cell_type":"code","source":"dataset","metadata":{"execution":{"iopub.status.busy":"2024-05-18T10:50:59.937376Z","iopub.execute_input":"2024-05-18T10:50:59.937812Z","iopub.status.idle":"2024-05-18T10:50:59.944844Z","shell.execute_reply.started":"2024-05-18T10:50:59.937785Z","shell.execute_reply":"2024-05-18T10:50:59.943916Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['question_type', 'multiple_choice_answer', 'answers', 'image_id', 'answer_type', 'question_id', 'question', 'image'],\n    num_rows: 110939\n})"},"metadata":{}}]},{"cell_type":"code","source":"dataset[0]","metadata":{"execution":{"iopub.status.busy":"2024-05-18T10:50:59.946878Z","iopub.execute_input":"2024-05-18T10:50:59.947208Z","iopub.status.idle":"2024-05-18T10:50:59.985434Z","shell.execute_reply.started":"2024-05-18T10:50:59.947179Z","shell.execute_reply":"2024-05-18T10:50:59.984449Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"{'question_type': 'what is this',\n 'multiple_choice_answer': 'net',\n 'answers': [{'answer': 'net', 'answer_confidence': 'maybe', 'answer_id': 1},\n  {'answer': 'net', 'answer_confidence': 'yes', 'answer_id': 2},\n  {'answer': 'net', 'answer_confidence': 'yes', 'answer_id': 3},\n  {'answer': 'netting', 'answer_confidence': 'yes', 'answer_id': 4},\n  {'answer': 'net', 'answer_confidence': 'yes', 'answer_id': 5},\n  {'answer': 'net', 'answer_confidence': 'yes', 'answer_id': 6},\n  {'answer': 'mesh', 'answer_confidence': 'maybe', 'answer_id': 7},\n  {'answer': 'net', 'answer_confidence': 'yes', 'answer_id': 8},\n  {'answer': 'net', 'answer_confidence': 'yes', 'answer_id': 9},\n  {'answer': 'net', 'answer_confidence': 'yes', 'answer_id': 10}],\n 'image_id': 458752,\n 'answer_type': 'other',\n 'question_id': 458752000,\n 'question': 'What is this photo taken looking through?',\n 'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=640x480>}"},"metadata":{}}]},{"cell_type":"code","source":"dataset[2000]['image']","metadata":{"execution":{"iopub.status.busy":"2024-05-18T10:50:59.986558Z","iopub.execute_input":"2024-05-18T10:50:59.986857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install pytorch-pretrained-vit\n!pip install vit-pytorch","metadata":{"execution":{"iopub.status.idle":"2024-05-18T10:51:29.647773Z","shell.execute_reply.started":"2024-05-18T10:51:00.078510Z","shell.execute_reply":"2024-05-18T10:51:29.646625Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Collecting pytorch-pretrained-vit\n  Downloading pytorch-pretrained-vit-0.0.7.tar.gz (13 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from pytorch-pretrained-vit) (2.1.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->pytorch-pretrained-vit) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->pytorch-pretrained-vit) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->pytorch-pretrained-vit) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->pytorch-pretrained-vit) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->pytorch-pretrained-vit) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->pytorch-pretrained-vit) (2024.2.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->pytorch-pretrained-vit) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->pytorch-pretrained-vit) (1.3.0)\nBuilding wheels for collected packages: pytorch-pretrained-vit\n  Building wheel for pytorch-pretrained-vit (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pytorch-pretrained-vit: filename=pytorch_pretrained_vit-0.0.7-py3-none-any.whl size=11116 sha256=b1c71f48246314f7db431d176107a99cb8bdacd0b1a366b09d725046ae165099\n  Stored in directory: /root/.cache/pip/wheels/2d/46/ad/12007be9d377d0fbf27ef75b6e47ed92832ab6b70dbf004b6f\nSuccessfully built pytorch-pretrained-vit\nInstalling collected packages: pytorch-pretrained-vit\nSuccessfully installed pytorch-pretrained-vit-0.0.7\nCollecting vit-pytorch\n  Downloading vit_pytorch-1.6.9-py3-none-any.whl.metadata (65 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.7/65.7 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hCollecting einops>=0.7.0 (from vit-pytorch)\n  Downloading einops-0.8.0-py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: torch>=1.10 in /opt/conda/lib/python3.10/site-packages (from vit-pytorch) (2.1.2)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from vit-pytorch) (0.16.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.10->vit-pytorch) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.10->vit-pytorch) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10->vit-pytorch) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10->vit-pytorch) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10->vit-pytorch) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.10->vit-pytorch) (2024.2.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision->vit-pytorch) (1.26.4)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchvision->vit-pytorch) (2.31.0)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision->vit-pytorch) (9.5.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10->vit-pytorch) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision->vit-pytorch) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision->vit-pytorch) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision->vit-pytorch) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision->vit-pytorch) (2024.2.2)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10->vit-pytorch) (1.3.0)\nDownloading vit_pytorch-1.6.9-py3-none-any.whl (119 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.7/119.7 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading einops-0.8.0-py3-none-any.whl (43 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: einops, vit-pytorch\nSuccessfully installed einops-0.8.0 vit-pytorch-1.6.9\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom transformers import BertModel, BertTokenizer\nfrom vit_pytorch import ViT\nfrom torch.utils.data import DataLoader, Dataset\nimport numpy as np\nfrom PIL import Image\nimport torchvision.transforms as transforms\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')","metadata":{"execution":{"iopub.status.busy":"2024-05-18T10:56:30.912653Z","iopub.execute_input":"2024-05-18T10:56:30.913501Z","iopub.status.idle":"2024-05-18T10:56:40.929031Z","shell.execute_reply.started":"2024-05-18T10:56:30.913461Z","shell.execute_reply":"2024-05-18T10:56:40.928187Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"17cc9ee859a54051bfd8aa4b16b87732"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f86e8a61a044c0faf0b78236a956846"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"410e4d3d548347ce9f223fe144e4201e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"80bb3a68be5e4a42a8e659a8fd89a39a"}},"metadata":{}}]},{"cell_type":"code","source":"class VQADataset(Dataset):\n    def __init__(self, dataset, tokenizer):\n        self.dataset = dataset\n        self.tokenizer = tokenizer\n\n    def __len__(self):\n        return len(self.dataset)\n\n    def __getitem__(self, idx):\n        item = self.dataset[idx]\n        image_path = item[\"image\"]\n        question = item[\"question\"]\n        label = item[\"multiple_choice_answer\"]\n\n        # Preprocess image\n        image = preprocess_image(image_path)\n        # Tokenize inputs\n        inputs = self.tokenizer(question, return_tensors=\"pt\", padding=True, truncation=True, max_length=64)\n        input_ids = inputs[\"input_ids\"].squeeze(0)\n\n        # Create attention mask with the same shape as input_ids\n        attention_mask = (input_ids != 0).long()\n        return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"image\": image, \"label\": label}\n","metadata":{"execution":{"iopub.status.busy":"2024-05-18T10:56:44.402402Z","iopub.execute_input":"2024-05-18T10:56:44.402958Z","iopub.status.idle":"2024-05-18T10:56:44.410666Z","shell.execute_reply.started":"2024-05-18T10:56:44.402928Z","shell.execute_reply":"2024-05-18T10:56:44.409718Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"print(tokenizer.vocab_size)","metadata":{"execution":{"iopub.status.busy":"2024-05-18T10:56:44.641554Z","iopub.execute_input":"2024-05-18T10:56:44.641921Z","iopub.status.idle":"2024-05-18T10:56:44.646968Z","shell.execute_reply.started":"2024-05-18T10:56:44.641892Z","shell.execute_reply":"2024-05-18T10:56:44.645990Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"30522\n","output_type":"stream"}]},{"cell_type":"code","source":"def preprocess_image(image_path):\n    image = image_path\n    if image.mode != 'RGB':\n        image = image.convert('RGB')\n    transform = transforms.Compose([\n        transforms.Resize((224, 224)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    ])\n    image = transform(image)\n    return image\n\ndef show_image(image):\n    # Denormalize the image\n    mean = np.array([0.485, 0.456, 0.406])\n    std = np.array([0.229, 0.224, 0.225])\n    \n    # Convert tensor to numpy array\n    image = image.cpu().numpy().transpose(1, 2, 0)\n    \n    # Denormalize\n    image = std * image + mean\n    image = np.clip(image, 0, 1)\n\n    # Show the image\n    plt.imshow(image)\n    plt.axis('off')\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-05-18T10:56:49.990054Z","iopub.execute_input":"2024-05-18T10:56:49.990709Z","iopub.status.idle":"2024-05-18T10:56:49.998347Z","shell.execute_reply.started":"2024-05-18T10:56:49.990680Z","shell.execute_reply":"2024-05-18T10:56:49.997430Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"for i, batch in enumerate(dataloader):\n    if i %1000:\n        continue\n    else:\n        input_ids = batch[\"input_ids\"].to(device)\n        attention_mask = batch[\"attention_mask\"].to(device)\n        image = batch[\"image\"].to(device)\n        question = tokenizer.decode(input_ids.squeeze().tolist())  # Convert input_ids to string question\n\n        # Debug: Print shape of the image tensor\n        print(\"Image shape:\", image.shape)\n\n        show_image(image.squeeze())\n\n        print(\"Question:\", question)\n        print(\"Answer:\", batch['label'])","metadata":{"execution":{"iopub.status.busy":"2024-05-18T03:57:05.962403Z","iopub.execute_input":"2024-05-18T03:57:05.962736Z","iopub.status.idle":"2024-05-18T03:57:06.008283Z","shell.execute_reply.started":"2024-05-18T03:57:05.962706Z","shell.execute_reply":"2024-05-18T03:57:06.007059Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":16,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[43mdataloader\u001b[49m):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m\u001b[38;5;241m1000\u001b[39m:\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n","\u001b[0;31mNameError\u001b[0m: name 'dataloader' is not defined"],"ename":"NameError","evalue":"name 'dataloader' is not defined","output_type":"error"}]},{"cell_type":"markdown","source":" # LoRA finetunig","metadata":{}},{"cell_type":"code","source":"!pip install peft","metadata":{"execution":{"iopub.status.busy":"2024-05-18T10:57:03.732733Z","iopub.execute_input":"2024-05-18T10:57:03.733104Z","iopub.status.idle":"2024-05-18T10:57:16.461306Z","shell.execute_reply.started":"2024-05-18T10:57:03.733077Z","shell.execute_reply":"2024-05-18T10:57:16.460298Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Collecting peft\n  Downloading peft-0.11.1-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft) (6.0.1)\nRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft) (2.1.2)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft) (4.39.3)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft) (4.66.1)\nRequirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.29.3)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft) (0.4.3)\nRequirement already satisfied: huggingface-hub>=0.17.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.22.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (3.13.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2024.2.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2.31.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->peft) (3.1.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (2023.12.25)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (0.15.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.2.2)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\nDownloading peft-0.11.1-py3-none-any.whl (251 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.6/251.6 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: peft\nSuccessfully installed peft-0.11.1\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom transformers import BertModel, ViTModel\n\n# Load the models\nbert_model = BertModel.from_pretrained('bert-base-uncased')\nvit_model = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\n\n# Print module names in BERT model\nprint(\"BERT Model Modules:\")\nfor name, module in bert_model.named_modules():\n    print(name)\n\n# Print module names in ViT model\nprint(\"\\nViT Model Modules:\")\nfor name, module in vit_model.named_modules():\n    print(name)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-18T04:21:03.490396Z","iopub.execute_input":"2024-05-18T04:21:03.490797Z","iopub.status.idle":"2024-05-18T04:21:04.533290Z","shell.execute_reply.started":"2024-05-18T04:21:03.490768Z","shell.execute_reply":"2024-05-18T04:21:04.532314Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"BERT Model Modules:\n\nembeddings\nembeddings.word_embeddings\nembeddings.position_embeddings\nembeddings.token_type_embeddings\nembeddings.LayerNorm\nembeddings.dropout\nencoder\nencoder.layer\nencoder.layer.0\nencoder.layer.0.attention\nencoder.layer.0.attention.self\nencoder.layer.0.attention.self.query\nencoder.layer.0.attention.self.key\nencoder.layer.0.attention.self.value\nencoder.layer.0.attention.self.dropout\nencoder.layer.0.attention.output\nencoder.layer.0.attention.output.dense\nencoder.layer.0.attention.output.LayerNorm\nencoder.layer.0.attention.output.dropout\nencoder.layer.0.intermediate\nencoder.layer.0.intermediate.dense\nencoder.layer.0.intermediate.intermediate_act_fn\nencoder.layer.0.output\nencoder.layer.0.output.dense\nencoder.layer.0.output.LayerNorm\nencoder.layer.0.output.dropout\nencoder.layer.1\nencoder.layer.1.attention\nencoder.layer.1.attention.self\nencoder.layer.1.attention.self.query\nencoder.layer.1.attention.self.key\nencoder.layer.1.attention.self.value\nencoder.layer.1.attention.self.dropout\nencoder.layer.1.attention.output\nencoder.layer.1.attention.output.dense\nencoder.layer.1.attention.output.LayerNorm\nencoder.layer.1.attention.output.dropout\nencoder.layer.1.intermediate\nencoder.layer.1.intermediate.dense\nencoder.layer.1.intermediate.intermediate_act_fn\nencoder.layer.1.output\nencoder.layer.1.output.dense\nencoder.layer.1.output.LayerNorm\nencoder.layer.1.output.dropout\nencoder.layer.2\nencoder.layer.2.attention\nencoder.layer.2.attention.self\nencoder.layer.2.attention.self.query\nencoder.layer.2.attention.self.key\nencoder.layer.2.attention.self.value\nencoder.layer.2.attention.self.dropout\nencoder.layer.2.attention.output\nencoder.layer.2.attention.output.dense\nencoder.layer.2.attention.output.LayerNorm\nencoder.layer.2.attention.output.dropout\nencoder.layer.2.intermediate\nencoder.layer.2.intermediate.dense\nencoder.layer.2.intermediate.intermediate_act_fn\nencoder.layer.2.output\nencoder.layer.2.output.dense\nencoder.layer.2.output.LayerNorm\nencoder.layer.2.output.dropout\nencoder.layer.3\nencoder.layer.3.attention\nencoder.layer.3.attention.self\nencoder.layer.3.attention.self.query\nencoder.layer.3.attention.self.key\nencoder.layer.3.attention.self.value\nencoder.layer.3.attention.self.dropout\nencoder.layer.3.attention.output\nencoder.layer.3.attention.output.dense\nencoder.layer.3.attention.output.LayerNorm\nencoder.layer.3.attention.output.dropout\nencoder.layer.3.intermediate\nencoder.layer.3.intermediate.dense\nencoder.layer.3.intermediate.intermediate_act_fn\nencoder.layer.3.output\nencoder.layer.3.output.dense\nencoder.layer.3.output.LayerNorm\nencoder.layer.3.output.dropout\nencoder.layer.4\nencoder.layer.4.attention\nencoder.layer.4.attention.self\nencoder.layer.4.attention.self.query\nencoder.layer.4.attention.self.key\nencoder.layer.4.attention.self.value\nencoder.layer.4.attention.self.dropout\nencoder.layer.4.attention.output\nencoder.layer.4.attention.output.dense\nencoder.layer.4.attention.output.LayerNorm\nencoder.layer.4.attention.output.dropout\nencoder.layer.4.intermediate\nencoder.layer.4.intermediate.dense\nencoder.layer.4.intermediate.intermediate_act_fn\nencoder.layer.4.output\nencoder.layer.4.output.dense\nencoder.layer.4.output.LayerNorm\nencoder.layer.4.output.dropout\nencoder.layer.5\nencoder.layer.5.attention\nencoder.layer.5.attention.self\nencoder.layer.5.attention.self.query\nencoder.layer.5.attention.self.key\nencoder.layer.5.attention.self.value\nencoder.layer.5.attention.self.dropout\nencoder.layer.5.attention.output\nencoder.layer.5.attention.output.dense\nencoder.layer.5.attention.output.LayerNorm\nencoder.layer.5.attention.output.dropout\nencoder.layer.5.intermediate\nencoder.layer.5.intermediate.dense\nencoder.layer.5.intermediate.intermediate_act_fn\nencoder.layer.5.output\nencoder.layer.5.output.dense\nencoder.layer.5.output.LayerNorm\nencoder.layer.5.output.dropout\nencoder.layer.6\nencoder.layer.6.attention\nencoder.layer.6.attention.self\nencoder.layer.6.attention.self.query\nencoder.layer.6.attention.self.key\nencoder.layer.6.attention.self.value\nencoder.layer.6.attention.self.dropout\nencoder.layer.6.attention.output\nencoder.layer.6.attention.output.dense\nencoder.layer.6.attention.output.LayerNorm\nencoder.layer.6.attention.output.dropout\nencoder.layer.6.intermediate\nencoder.layer.6.intermediate.dense\nencoder.layer.6.intermediate.intermediate_act_fn\nencoder.layer.6.output\nencoder.layer.6.output.dense\nencoder.layer.6.output.LayerNorm\nencoder.layer.6.output.dropout\nencoder.layer.7\nencoder.layer.7.attention\nencoder.layer.7.attention.self\nencoder.layer.7.attention.self.query\nencoder.layer.7.attention.self.key\nencoder.layer.7.attention.self.value\nencoder.layer.7.attention.self.dropout\nencoder.layer.7.attention.output\nencoder.layer.7.attention.output.dense\nencoder.layer.7.attention.output.LayerNorm\nencoder.layer.7.attention.output.dropout\nencoder.layer.7.intermediate\nencoder.layer.7.intermediate.dense\nencoder.layer.7.intermediate.intermediate_act_fn\nencoder.layer.7.output\nencoder.layer.7.output.dense\nencoder.layer.7.output.LayerNorm\nencoder.layer.7.output.dropout\nencoder.layer.8\nencoder.layer.8.attention\nencoder.layer.8.attention.self\nencoder.layer.8.attention.self.query\nencoder.layer.8.attention.self.key\nencoder.layer.8.attention.self.value\nencoder.layer.8.attention.self.dropout\nencoder.layer.8.attention.output\nencoder.layer.8.attention.output.dense\nencoder.layer.8.attention.output.LayerNorm\nencoder.layer.8.attention.output.dropout\nencoder.layer.8.intermediate\nencoder.layer.8.intermediate.dense\nencoder.layer.8.intermediate.intermediate_act_fn\nencoder.layer.8.output\nencoder.layer.8.output.dense\nencoder.layer.8.output.LayerNorm\nencoder.layer.8.output.dropout\nencoder.layer.9\nencoder.layer.9.attention\nencoder.layer.9.attention.self\nencoder.layer.9.attention.self.query\nencoder.layer.9.attention.self.key\nencoder.layer.9.attention.self.value\nencoder.layer.9.attention.self.dropout\nencoder.layer.9.attention.output\nencoder.layer.9.attention.output.dense\nencoder.layer.9.attention.output.LayerNorm\nencoder.layer.9.attention.output.dropout\nencoder.layer.9.intermediate\nencoder.layer.9.intermediate.dense\nencoder.layer.9.intermediate.intermediate_act_fn\nencoder.layer.9.output\nencoder.layer.9.output.dense\nencoder.layer.9.output.LayerNorm\nencoder.layer.9.output.dropout\nencoder.layer.10\nencoder.layer.10.attention\nencoder.layer.10.attention.self\nencoder.layer.10.attention.self.query\nencoder.layer.10.attention.self.key\nencoder.layer.10.attention.self.value\nencoder.layer.10.attention.self.dropout\nencoder.layer.10.attention.output\nencoder.layer.10.attention.output.dense\nencoder.layer.10.attention.output.LayerNorm\nencoder.layer.10.attention.output.dropout\nencoder.layer.10.intermediate\nencoder.layer.10.intermediate.dense\nencoder.layer.10.intermediate.intermediate_act_fn\nencoder.layer.10.output\nencoder.layer.10.output.dense\nencoder.layer.10.output.LayerNorm\nencoder.layer.10.output.dropout\nencoder.layer.11\nencoder.layer.11.attention\nencoder.layer.11.attention.self\nencoder.layer.11.attention.self.query\nencoder.layer.11.attention.self.key\nencoder.layer.11.attention.self.value\nencoder.layer.11.attention.self.dropout\nencoder.layer.11.attention.output\nencoder.layer.11.attention.output.dense\nencoder.layer.11.attention.output.LayerNorm\nencoder.layer.11.attention.output.dropout\nencoder.layer.11.intermediate\nencoder.layer.11.intermediate.dense\nencoder.layer.11.intermediate.intermediate_act_fn\nencoder.layer.11.output\nencoder.layer.11.output.dense\nencoder.layer.11.output.LayerNorm\nencoder.layer.11.output.dropout\npooler\npooler.dense\npooler.activation\n\nViT Model Modules:\n\nembeddings\nembeddings.patch_embeddings\nembeddings.patch_embeddings.projection\nembeddings.dropout\nencoder\nencoder.layer\nencoder.layer.0\nencoder.layer.0.attention\nencoder.layer.0.attention.attention\nencoder.layer.0.attention.attention.query\nencoder.layer.0.attention.attention.key\nencoder.layer.0.attention.attention.value\nencoder.layer.0.attention.attention.dropout\nencoder.layer.0.attention.output\nencoder.layer.0.attention.output.dense\nencoder.layer.0.attention.output.dropout\nencoder.layer.0.intermediate\nencoder.layer.0.intermediate.dense\nencoder.layer.0.intermediate.intermediate_act_fn\nencoder.layer.0.output\nencoder.layer.0.output.dense\nencoder.layer.0.output.dropout\nencoder.layer.0.layernorm_before\nencoder.layer.0.layernorm_after\nencoder.layer.1\nencoder.layer.1.attention\nencoder.layer.1.attention.attention\nencoder.layer.1.attention.attention.query\nencoder.layer.1.attention.attention.key\nencoder.layer.1.attention.attention.value\nencoder.layer.1.attention.attention.dropout\nencoder.layer.1.attention.output\nencoder.layer.1.attention.output.dense\nencoder.layer.1.attention.output.dropout\nencoder.layer.1.intermediate\nencoder.layer.1.intermediate.dense\nencoder.layer.1.intermediate.intermediate_act_fn\nencoder.layer.1.output\nencoder.layer.1.output.dense\nencoder.layer.1.output.dropout\nencoder.layer.1.layernorm_before\nencoder.layer.1.layernorm_after\nencoder.layer.2\nencoder.layer.2.attention\nencoder.layer.2.attention.attention\nencoder.layer.2.attention.attention.query\nencoder.layer.2.attention.attention.key\nencoder.layer.2.attention.attention.value\nencoder.layer.2.attention.attention.dropout\nencoder.layer.2.attention.output\nencoder.layer.2.attention.output.dense\nencoder.layer.2.attention.output.dropout\nencoder.layer.2.intermediate\nencoder.layer.2.intermediate.dense\nencoder.layer.2.intermediate.intermediate_act_fn\nencoder.layer.2.output\nencoder.layer.2.output.dense\nencoder.layer.2.output.dropout\nencoder.layer.2.layernorm_before\nencoder.layer.2.layernorm_after\nencoder.layer.3\nencoder.layer.3.attention\nencoder.layer.3.attention.attention\nencoder.layer.3.attention.attention.query\nencoder.layer.3.attention.attention.key\nencoder.layer.3.attention.attention.value\nencoder.layer.3.attention.attention.dropout\nencoder.layer.3.attention.output\nencoder.layer.3.attention.output.dense\nencoder.layer.3.attention.output.dropout\nencoder.layer.3.intermediate\nencoder.layer.3.intermediate.dense\nencoder.layer.3.intermediate.intermediate_act_fn\nencoder.layer.3.output\nencoder.layer.3.output.dense\nencoder.layer.3.output.dropout\nencoder.layer.3.layernorm_before\nencoder.layer.3.layernorm_after\nencoder.layer.4\nencoder.layer.4.attention\nencoder.layer.4.attention.attention\nencoder.layer.4.attention.attention.query\nencoder.layer.4.attention.attention.key\nencoder.layer.4.attention.attention.value\nencoder.layer.4.attention.attention.dropout\nencoder.layer.4.attention.output\nencoder.layer.4.attention.output.dense\nencoder.layer.4.attention.output.dropout\nencoder.layer.4.intermediate\nencoder.layer.4.intermediate.dense\nencoder.layer.4.intermediate.intermediate_act_fn\nencoder.layer.4.output\nencoder.layer.4.output.dense\nencoder.layer.4.output.dropout\nencoder.layer.4.layernorm_before\nencoder.layer.4.layernorm_after\nencoder.layer.5\nencoder.layer.5.attention\nencoder.layer.5.attention.attention\nencoder.layer.5.attention.attention.query\nencoder.layer.5.attention.attention.key\nencoder.layer.5.attention.attention.value\nencoder.layer.5.attention.attention.dropout\nencoder.layer.5.attention.output\nencoder.layer.5.attention.output.dense\nencoder.layer.5.attention.output.dropout\nencoder.layer.5.intermediate\nencoder.layer.5.intermediate.dense\nencoder.layer.5.intermediate.intermediate_act_fn\nencoder.layer.5.output\nencoder.layer.5.output.dense\nencoder.layer.5.output.dropout\nencoder.layer.5.layernorm_before\nencoder.layer.5.layernorm_after\nencoder.layer.6\nencoder.layer.6.attention\nencoder.layer.6.attention.attention\nencoder.layer.6.attention.attention.query\nencoder.layer.6.attention.attention.key\nencoder.layer.6.attention.attention.value\nencoder.layer.6.attention.attention.dropout\nencoder.layer.6.attention.output\nencoder.layer.6.attention.output.dense\nencoder.layer.6.attention.output.dropout\nencoder.layer.6.intermediate\nencoder.layer.6.intermediate.dense\nencoder.layer.6.intermediate.intermediate_act_fn\nencoder.layer.6.output\nencoder.layer.6.output.dense\nencoder.layer.6.output.dropout\nencoder.layer.6.layernorm_before\nencoder.layer.6.layernorm_after\nencoder.layer.7\nencoder.layer.7.attention\nencoder.layer.7.attention.attention\nencoder.layer.7.attention.attention.query\nencoder.layer.7.attention.attention.key\nencoder.layer.7.attention.attention.value\nencoder.layer.7.attention.attention.dropout\nencoder.layer.7.attention.output\nencoder.layer.7.attention.output.dense\nencoder.layer.7.attention.output.dropout\nencoder.layer.7.intermediate\nencoder.layer.7.intermediate.dense\nencoder.layer.7.intermediate.intermediate_act_fn\nencoder.layer.7.output\nencoder.layer.7.output.dense\nencoder.layer.7.output.dropout\nencoder.layer.7.layernorm_before\nencoder.layer.7.layernorm_after\nencoder.layer.8\nencoder.layer.8.attention\nencoder.layer.8.attention.attention\nencoder.layer.8.attention.attention.query\nencoder.layer.8.attention.attention.key\nencoder.layer.8.attention.attention.value\nencoder.layer.8.attention.attention.dropout\nencoder.layer.8.attention.output\nencoder.layer.8.attention.output.dense\nencoder.layer.8.attention.output.dropout\nencoder.layer.8.intermediate\nencoder.layer.8.intermediate.dense\nencoder.layer.8.intermediate.intermediate_act_fn\nencoder.layer.8.output\nencoder.layer.8.output.dense\nencoder.layer.8.output.dropout\nencoder.layer.8.layernorm_before\nencoder.layer.8.layernorm_after\nencoder.layer.9\nencoder.layer.9.attention\nencoder.layer.9.attention.attention\nencoder.layer.9.attention.attention.query\nencoder.layer.9.attention.attention.key\nencoder.layer.9.attention.attention.value\nencoder.layer.9.attention.attention.dropout\nencoder.layer.9.attention.output\nencoder.layer.9.attention.output.dense\nencoder.layer.9.attention.output.dropout\nencoder.layer.9.intermediate\nencoder.layer.9.intermediate.dense\nencoder.layer.9.intermediate.intermediate_act_fn\nencoder.layer.9.output\nencoder.layer.9.output.dense\nencoder.layer.9.output.dropout\nencoder.layer.9.layernorm_before\nencoder.layer.9.layernorm_after\nencoder.layer.10\nencoder.layer.10.attention\nencoder.layer.10.attention.attention\nencoder.layer.10.attention.attention.query\nencoder.layer.10.attention.attention.key\nencoder.layer.10.attention.attention.value\nencoder.layer.10.attention.attention.dropout\nencoder.layer.10.attention.output\nencoder.layer.10.attention.output.dense\nencoder.layer.10.attention.output.dropout\nencoder.layer.10.intermediate\nencoder.layer.10.intermediate.dense\nencoder.layer.10.intermediate.intermediate_act_fn\nencoder.layer.10.output\nencoder.layer.10.output.dense\nencoder.layer.10.output.dropout\nencoder.layer.10.layernorm_before\nencoder.layer.10.layernorm_after\nencoder.layer.11\nencoder.layer.11.attention\nencoder.layer.11.attention.attention\nencoder.layer.11.attention.attention.query\nencoder.layer.11.attention.attention.key\nencoder.layer.11.attention.attention.value\nencoder.layer.11.attention.attention.dropout\nencoder.layer.11.attention.output\nencoder.layer.11.attention.output.dense\nencoder.layer.11.attention.output.dropout\nencoder.layer.11.intermediate\nencoder.layer.11.intermediate.dense\nencoder.layer.11.intermediate.intermediate_act_fn\nencoder.layer.11.output\nencoder.layer.11.output.dense\nencoder.layer.11.output.dropout\nencoder.layer.11.layernorm_before\nencoder.layer.11.layernorm_after\nlayernorm\npooler\npooler.dense\npooler.activation\n","output_type":"stream"}]},{"cell_type":"code","source":"%%time\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, random_split\nfrom tqdm import tqdm\nfrom transformers import BertModel, ViTModel, BertTokenizer\n\n# Assuming dataset and VQADataset are already defined\nsmall_dataset = dataset.select(range(10000))\n\n# Split the dataset into training, validation, and test sets\ntrain_size = int(0.8 * len(small_dataset))\nval_size = int(0.1 * len(small_dataset))\ntest_size = len(small_dataset) - train_size - val_size\n\ntrain_dataset, val_dataset, test_dataset = random_split(small_dataset, [train_size, val_size, test_size])\n\n# Create VQADataset instances\ntrain_vqa_dataset = VQADataset(train_dataset, tokenizer)\nval_vqa_dataset = VQADataset(val_dataset, tokenizer)\ntest_vqa_dataset = VQADataset(test_dataset, tokenizer)\n\n# Custom collate function\ndef custom_collate_fn(batch):\n    max_seq_length = max(len(item[\"input_ids\"]) for item in batch)\n    \n    padded_input_ids = []\n    padded_attention_masks = []\n    images = []\n    labels = []\n    \n    for item in batch:\n        input_ids = item[\"input_ids\"]\n        attention_mask = item[\"attention_mask\"]\n        image = item[\"image\"]\n        label = item[\"label\"]\n        \n        # Pad input_ids and attention_mask\n        padding_length = max_seq_length - len(input_ids)\n        padded_input_ids.append(torch.cat([input_ids, torch.zeros(padding_length, dtype=torch.long)]))\n        padded_attention_masks.append(torch.cat([attention_mask, torch.zeros(padding_length, dtype=torch.long)]))\n        \n        images.append(image)\n        \n        # Encode labels\n        encoded_label = torch.tensor(tokenizer.convert_tokens_to_ids(label))\n        labels.append(encoded_label)\n    \n    padded_input_ids = torch.stack(padded_input_ids)\n    padded_attention_masks = torch.stack(padded_attention_masks)\n    images = torch.stack(images)\n    labels = torch.stack(labels)  # No need for this to be a list\n    \n    return {\n        \"input_ids\": padded_input_ids,\n        \"attention_mask\": padded_attention_masks,\n        \"image\": images,\n        \"label\": labels\n    }\n\n# Create DataLoader instances with custom collate function\ntrain_loader = DataLoader(train_vqa_dataset, batch_size=64, shuffle=True, collate_fn=custom_collate_fn)\nval_loader = DataLoader(val_vqa_dataset, batch_size=64, shuffle=False, collate_fn=custom_collate_fn)\ntest_loader = DataLoader(test_vqa_dataset, batch_size=64, shuffle=False, collate_fn=custom_collate_fn)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-18T10:57:39.364891Z","iopub.execute_input":"2024-05-18T10:57:39.365400Z","iopub.status.idle":"2024-05-18T10:57:39.412098Z","shell.execute_reply.started":"2024-05-18T10:57:39.365366Z","shell.execute_reply":"2024-05-18T10:57:39.411183Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"CPU times: user 12.1 ms, sys: 3.02 ms, total: 15.1 ms\nWall time: 32.2 ms\n","output_type":"stream"}]},{"cell_type":"code","source":"%%time\nimport torch\nimport torch.nn as nn\nfrom transformers import BertModel, ViTModel, BertTokenizer\nfrom peft import LoraConfig, get_peft_model\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\n\nclass VQAModel(nn.Module):\n    def __init__(self, image_model, text_model, vocab_size):\n        super(VQAModel, self).__init__()\n        self.image_model = image_model\n        self.text_model = text_model\n        \n        # LoRA configuration\n        self.lora_config = LoraConfig(r=16, lora_alpha=32, lora_dropout=0.1, bias=\"none\")\n        \n        # Apply LoRA to BERT and ViT models\n        self.text_model = get_peft_model(\n            self.text_model,\n            self.lora_config\n        )\n        self.image_model = get_peft_model(\n            self.image_model,\n            self.lora_config\n        )\n\n        # Transformer decoder layer\n        self.decoder_layer = nn.TransformerDecoderLayer(d_model=768, nhead=8)\n        self.transformer_decoder = nn.TransformerDecoder(self.decoder_layer, num_layers=6)\n\n        # Final classification layer\n        self.fc = nn.Linear(768, vocab_size)\n\n    def forward(self, image, input_ids, attention_mask):\n        # Extract image features using ViT\n        image_output = self.image_model(pixel_values=image).last_hidden_state  # Shape: [batch_size, seq_len, 768]\n        image_cls_token = image_output[:, 0, :]  # Use [CLS] token representation for the image\n\n        # Expand image features to match the batch size\n        weighted_image_output = image_cls_token.unsqueeze(1)  # [batch_size, 1, 768]\n\n        # Extract text features\n        text_output = self.text_model(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state\n\n        # Prepare for transformer decoder\n        image_features = weighted_image_output.permute(1, 0, 2)  # [1, batch_size, 768]\n        text_features = text_output.permute(1, 0, 2)  # [seq_len, batch_size, 768]\n\n        # Transformer decoder forward pass\n        decoder_output = self.transformer_decoder(tgt=text_features, memory=image_features)\n\n        # Classification\n        combined_output = decoder_output.permute(1, 0, 2)  # [batch_size, seq_len, 768]\n        logits = self.fc(combined_output[:, -1, :])  # Use the last token's representation for classification\n\n        return logits\n\n# Set up the model, optimizer, and loss function\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nimage_model = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\ntext_model = BertModel.from_pretrained('bert-base-uncased')\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nvocab_size = tokenizer.vocab_size\n\nmodel = VQAModel(image_model, text_model, vocab_size).to(device)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\ncriterion = nn.CrossEntropyLoss()\n\n# Training loop\nepochs =15\nfor epoch in range(epochs):\n    model.train()\n    total_loss = 0.0\n    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n        input_ids = batch[\"input_ids\"].to(device)\n        attention_mask = batch[\"attention_mask\"].to(device)\n        image = batch[\"image\"].to(device)\n        labels = batch[\"label\"].to(device)\n\n        optimizer.zero_grad()\n        outputs = model(image, input_ids, attention_mask)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n\n    print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(train_loader):.4f}\")\n\n# Evaluation loop\n    model.eval()\n    all_predictions = []\n    all_labels = []\n    with torch.no_grad():\n        for batch in tqdm(val_loader, desc=\"Validation\"):\n            input_ids = batch[\"input_ids\"].to(device)\n            attention_mask = batch[\"attention_mask\"].to(device)\n            image = batch[\"image\"].to(device)\n            labels = batch[\"label\"]\n\n            outputs = model(image, input_ids, attention_mask)\n            predictions = torch.argmax(outputs, dim=-1).tolist()\n            all_predictions.extend(predictions)\n            all_labels.extend(labels.tolist())\n\n    # Compute accuracy\n    correct = sum(1 for pred, label in zip(all_predictions, all_labels) if pred == label)\n    accuracy = correct / len(all_labels)\n    print(f\"Validation Accuracy: {accuracy:.4f}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-05-18T10:58:13.353102Z","iopub.execute_input":"2024-05-18T10:58:13.353506Z","iopub.status.idle":"2024-05-18T11:45:54.085050Z","shell.execute_reply.started":"2024-05-18T10:58:13.353478Z","shell.execute_reply":"2024-05-18T11:45:54.084052Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"Epoch 1/15: 100%|██████████| 125/125 [03:26<00:00,  1.65s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/15, Loss: 4.4449\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 16/16 [00:14<00:00,  1.07it/s]\n","output_type":"stream"},{"name":"stdout","text":"Validation Accuracy: 0.3480\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/15: 100%|██████████| 125/125 [02:55<00:00,  1.41s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/15, Loss: 3.0447\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 16/16 [00:14<00:00,  1.13it/s]\n","output_type":"stream"},{"name":"stdout","text":"Validation Accuracy: 0.3810\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/15: 100%|██████████| 125/125 [02:55<00:00,  1.41s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/15, Loss: 2.6383\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 16/16 [00:14<00:00,  1.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"Validation Accuracy: 0.3930\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/15: 100%|██████████| 125/125 [02:54<00:00,  1.40s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4/15, Loss: 2.2218\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 16/16 [00:13<00:00,  1.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"Validation Accuracy: 0.4140\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/15: 100%|██████████| 125/125 [02:55<00:00,  1.40s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5/15, Loss: 1.8389\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 16/16 [00:13<00:00,  1.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"Validation Accuracy: 0.4230\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/15: 100%|██████████| 125/125 [02:54<00:00,  1.39s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6/15, Loss: 1.4853\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 16/16 [00:13<00:00,  1.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"Validation Accuracy: 0.4220\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/15: 100%|██████████| 125/125 [02:54<00:00,  1.39s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7/15, Loss: 1.1853\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 16/16 [00:13<00:00,  1.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"Validation Accuracy: 0.4120\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/15: 100%|██████████| 125/125 [02:53<00:00,  1.39s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8/15, Loss: 0.9174\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 16/16 [00:13<00:00,  1.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"Validation Accuracy: 0.4050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/15: 100%|██████████| 125/125 [02:54<00:00,  1.39s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9/15, Loss: 0.6950\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 16/16 [00:13<00:00,  1.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"Validation Accuracy: 0.3970\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/15: 100%|██████████| 125/125 [02:53<00:00,  1.39s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10/15, Loss: 0.5295\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 16/16 [00:13<00:00,  1.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"Validation Accuracy: 0.4100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 11/15: 100%|██████████| 125/125 [02:54<00:00,  1.39s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 11/15, Loss: 0.4199\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 16/16 [00:13<00:00,  1.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"Validation Accuracy: 0.3990\n","output_type":"stream"},{"name":"stderr","text":"Epoch 12/15: 100%|██████████| 125/125 [02:54<00:00,  1.39s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 12/15, Loss: 0.3420\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 16/16 [00:13<00:00,  1.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"Validation Accuracy: 0.3990\n","output_type":"stream"},{"name":"stderr","text":"Epoch 13/15: 100%|██████████| 125/125 [02:54<00:00,  1.39s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 13/15, Loss: 0.2840\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 16/16 [00:13<00:00,  1.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"Validation Accuracy: 0.4090\n","output_type":"stream"},{"name":"stderr","text":"Epoch 14/15: 100%|██████████| 125/125 [02:53<00:00,  1.39s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 14/15, Loss: 0.2527\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 16/16 [00:13<00:00,  1.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"Validation Accuracy: 0.4050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 15/15: 100%|██████████| 125/125 [02:54<00:00,  1.40s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 15/15, Loss: 0.2057\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 16/16 [00:13<00:00,  1.16it/s]","output_type":"stream"},{"name":"stdout","text":"Validation Accuracy: 0.3820\nCPU times: user 58min 16s, sys: 5min 25s, total: 1h 3min 42s\nWall time: 47min 40s\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"def decode_predictions(outputs, tokenizer):\n    probabilities = torch.softmax(outputs, dim=-1)\n    predicted_indices = torch.argmax(probabilities, dim=-1)\n    predictions = []\n    for indices in predicted_indices:\n        tokens = tokenizer.convert_ids_to_tokens(indices.tolist())\n        prediction = \"\".join(tokens)\n        predictions.append(prediction)\n    return predictions\n","metadata":{"execution":{"iopub.status.busy":"2024-05-18T11:47:19.125801Z","iopub.execute_input":"2024-05-18T11:47:19.126691Z","iopub.status.idle":"2024-05-18T11:47:19.132043Z","shell.execute_reply.started":"2024-05-18T11:47:19.126659Z","shell.execute_reply":"2024-05-18T11:47:19.131096Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"from collections import defaultdict\n\ndef compute_accuracy(predictions, labels):\n    correct = sum(1 for pred, label in zip(predictions, labels) if pred == label)\n    total = len(labels)\n    return correct / total\n\n\ndef compute_precision_recall_f1(predictions, labels):\n    class_tp = defaultdict(int)\n    class_fp = defaultdict(int)\n    class_fn = defaultdict(int)\n\n    for pred, label in zip(predictions, labels):\n        if pred == label:\n            class_tp[label] += 1\n        else:\n            class_fp[pred] += 1\n            class_fn[label] += 1\n\n    all_classes = set(class_tp.keys()).union(class_fp.keys()).union(class_fn.keys())\n\n    precisions = []\n    recalls = []\n    f1_scores = []\n\n    for cls in all_classes:\n        tp = class_tp[cls]\n        fp = class_fp[cls]\n        fn = class_fn[cls]\n\n        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n\n        precisions.append(precision)\n        recalls.append(recall)\n        f1_scores.append(f1)\n\n    micro_precision = sum(class_tp.values()) / (sum(class_tp.values()) + sum(class_fp.values())) if (sum(class_tp.values()) + sum(class_fp.values())) > 0 else 0\n    micro_recall = sum(class_tp.values()) / (sum(class_tp.values()) + sum(class_fn.values())) if (sum(class_tp.values()) + sum(class_fn.values())) > 0 else 0\n    micro_f1 = 2 * micro_precision * micro_recall / (micro_precision + micro_recall) if (micro_precision + micro_recall) > 0 else 0\n\n    macro_precision = sum(precisions) / len(precisions) if precisions else 0\n    macro_recall = sum(recalls) / len(recalls) if recalls else 0\n    macro_f1 = sum(f1_scores) / len(f1_scores) if f1_scores else 0\n\n    return micro_precision, micro_recall, micro_f1, macro_precision, macro_recall, macro_f1\n","metadata":{"execution":{"iopub.status.busy":"2024-05-18T11:47:19.535179Z","iopub.execute_input":"2024-05-18T11:47:19.535563Z","iopub.status.idle":"2024-05-18T11:47:19.548865Z","shell.execute_reply.started":"2024-05-18T11:47:19.535533Z","shell.execute_reply":"2024-05-18T11:47:19.547908Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"print('hi')","metadata":{"execution":{"iopub.status.busy":"2024-05-18T10:57:52.052045Z","iopub.execute_input":"2024-05-18T10:57:52.052987Z","iopub.status.idle":"2024-05-18T10:57:52.057714Z","shell.execute_reply.started":"2024-05-18T10:57:52.052949Z","shell.execute_reply":"2024-05-18T10:57:52.056734Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"hi\n","output_type":"stream"}]},{"cell_type":"code","source":"# Set the model to evaluation mode\nmodel.eval()\n\n# Define lists to store predictions and ground truth labels\nall_predictions = []\nall_labels = []\n\n# Iterate through the test dataset\nwith torch.no_grad():\n    for batch in test_loader:\n        input_ids = batch[\"input_ids\"].to(device)\n        attention_mask = batch[\"attention_mask\"].to(device)\n        image = batch[\"image\"].to(device)\n        labels = batch[\"label\"]\n#         print(labels)\n        \n        # Forward pass\n        outputs = model(image, input_ids, attention_mask)\n#         print(torch.argmax(outputs, dim=-1))\n        # Decode predictions\n        predictions = decode_predictions(outputs,tokenizer) \n        lbl = []\n        for indices in labels:\n            tokens = tokenizer.convert_ids_to_tokens(indices.tolist())\n            prediction = \"\".join(tokens)\n            lbl.append(prediction)\n        # Collect predictions and ground truth labels\n        all_predictions.extend(predictions)\n        all_labels.extend(lbl)\n\n# Compute evaluation metrics\nprint(all_predictions)\nprint(all_labels)\naccuracy = compute_accuracy(np.array(all_predictions), np.array(all_labels))\nmicro_precision, micro_recall, micro_f1,macro_precision, macro_recall, macro_f1 = compute_precision_recall_f1(np.array(all_predictions), np.array(all_labels))\n\nprint(f\"Accuracy: {accuracy:.4f}\")\nprint(f\"Micro Precision: {micro_precision:.4f}\")\nprint(f\"Micro Recall: {micro_recall:.4f}\")\nprint(f\"Micro F1 Score: {micro_f1:.4f}\")\nprint(f\"Macro Precision: {micro_precision:.4f}\")\nprint(f\"Macro Recall: {micro_recall:.4f}\")\nprint(f\"Macro F1 Score: {micro_f1:.4f}\")","metadata":{"execution":{"iopub.status.busy":"2024-05-18T11:55:21.448781Z","iopub.execute_input":"2024-05-18T11:55:21.449487Z","iopub.status.idle":"2024-05-18T11:55:35.417721Z","shell.execute_reply.started":"2024-05-18T11:55:21.449455Z","shell.execute_reply":"2024-05-18T11:55:35.416774Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"['[UNK]', 'yes', 'thai', 'no', 'no', 'gray', '[UNK]', '[UNK]', 'yes', 'resting', 'tree', 'no', '6', 'yes', 'fork', '[UNK]', '1', 'cow', 'yes', '[UNK]', '[UNK]', 'water', 'cow', 'yes', '[UNK]', '[UNK]', 'no', '[UNK]', 'kitchen', '[UNK]', '2', 'no', 'yes', 'yes', 'blue', 'blue', '[UNK]', 'no', 'yes', '[UNK]', 'no', 'yes', 'yes', '7', 'blue', 'red', '0', 'yes', 'white', '0', '0', '6', 'bear', 'yellow', 'eating', 'boat', '[UNK]', '1', '2', 'no', '[UNK]', 'yes', '2', 'white', 'sleeping', 'yes', '[UNK]', '[UNK]', '[UNK]', 'phone', 'yes', '2', 'no', '[UNK]', '[UNK]', '7', 'outside', 'brown', '6', 'calf', '[UNK]', '[UNK]', '[UNK]', 'no', 'gray', '[UNK]', 'field', '[UNK]', 'yes', 'right', 'no', 'yes', 'blue', '[UNK]', '[UNK]', '[UNK]', 'no', 'yes', '[UNK]', '3', 'chair', '4', 'yes', 'no', 'snow', '4', 'yes', 'black', 'chocolate', 'brown', 'wood', 'cat', 'no', 'yes', 'yes', 'no', 'surfing', 'red', 'motorola', 'yes', 'red', '[UNK]', 'black', 'yes', 'leaves', 'tile', 'yes', '[UNK]', 'pizza', '2', 'no', 'yes', 'yes', 'no', 'tennis', 'brown', 'window', 'yes', 'yes', 'no', 'yes', '[UNK]', 'no', 'yes', 'yes', '[UNK]', '2', 'gray', 'table', '3', '8', '[UNK]', 'no', '[UNK]', '[UNK]', '[UNK]', 'yes', 'yes', 'dell', 'yes', 'pizza', '[UNK]', '3', '[UNK]', 'wood', 'yes', 'no', 'yes', '[UNK]', 'food', 'yes', 'yes', 'cow', '0', '[UNK]', 'red', 'white', 'tennis', 'yes', 'yes', '4', 'no', '[UNK]', '15', 'no', 'vent', 'greyhound', '2', 'no', '1', 'yes', '1', 'blue', '[UNK]', 'yes', 'no', 'real', '3', 'yes', 'no', 'yes', 'yes', 'no', 'car', 'blue', '[UNK]', 'white', 'no', 'vans', 'yes', 'no', 'yes', 'bathroom', 'left', '[UNK]', 'no', 'black', '[UNK]', 'yes', '[UNK]', 'white', 'yes', 'yes', 'christmas', 'yes', 'inside', '[UNK]', 'yes', 'blue', 'yes', '[UNK]', 'brown', 'yes', '2', 'skiing', '2', 'yes', 'yes', 'yes', 'yellow', 'camera', 'yes', '2', 'yes', '[UNK]', 'yes', 'pink', 'plane', 'no', 'metal', '[UNK]', 'wood', 'yes', 'no', 'no', 'truck', '2', '[UNK]', 'yes', 'yes', 'yellow', '2', '[UNK]', '1', '[UNK]', '[UNK]', 'no', '2', 'yes', 'standing', 'television', '[UNK]', 'kitchen', 'blue', 'yes', '[UNK]', 'yes', 'yes', 'no', 'white', 'sitting', 'left', 'white', '[UNK]', 'white', 'gray', 'yellow', 'no', 'silver', 'yes', 'yes', 'blue', 'yes', 'stop', 'yellow', '1', 'pizza', 'black', 'yes', 'banana', 'yellow', 'yes', 'no', 'africa', 'no', '2', 'yes', '[UNK]', 'daisy', 'yacht', 'no', '2', 'yes', 'white', '[UNK]', 'kitchen', '[UNK]', 'silver', '[UNK]', '2', 'yes', 'yes', 'jug', 'wood', 'yes', 'yes', 'train', '3', 'no', 'outside', 'no', '[UNK]', 'yes', 'blue', '[UNK]', 'no', 'no', '[UNK]', 'white', 'yes', 'sandals', 'yes', '[UNK]', 'blue', 'no', 'no', 'no', 'gray', 'yes', 'peach', 'no', '2', 'yes', 'yes', 'no', 'down', 'no', 'yes', 'salad', 'no', 'no', 'yes', 'yes', 'no', 'yes', '[UNK]', '[UNK]', '1', 'fruit', 'yes', '[UNK]', 'no', '2', 'umbrella', 'black', 'silver', 'log', 'yes', 'yes', 'green', 'bathroom', '[UNK]', 'suit', '[UNK]', 'yes', '[UNK]', 'brown', 'yes', 'no', 'yellow', 'yes', 'yes', 'yes', 'bird', '[UNK]', 'yes', 'black', '[UNK]', 'yes', 'no', '2', 'yes', 'gray', 'greyhound', '[UNK]', '[UNK]', '0', 'refrigerator', '[UNK]', '[UNK]', '10', 'girl', 'tracks', 'yes', 'no', '[UNK]', 'brace', 'green', 'many', 'yes', 'yes', 'kitchen', 'wall', '[UNK]', 'no', 'white', 'right', 'yes', 'yes', 'yes', 'floats', 'no', 'no', 'yes', 'no', '3', '4', 'yes', 'no', 'black', '[UNK]', 'wood', 'no', 'no', 'dell', '[UNK]', 'luggage', '2', 'no', 'yes', 'dell', 'black', 'yes', 'yes', 'yes', 'yellow', '2', 'no', 'no', '2', 'no', 'grass', '[UNK]', '[UNK]', '1', '[UNK]', '3', 'no', '[UNK]', 'yes', '[UNK]', 'yes', 'yes', '[UNK]', 'blue', 'bridge', '[UNK]', '1', '[UNK]', '[UNK]', 'yes', 'yes', 'blue', '1', 'no', 'black', 'yes', '2', '[UNK]', 'sand', 'no', '[UNK]', 'no', 'no', '[UNK]', 'daisy', '[UNK]', '[UNK]', '12', 'black', 'yellow', 'no', 'yes', '[UNK]', 'snow', '2', 'no', 'yes', 'yellow', '[UNK]', '1', 'yes', 'yes', 'yes', 'baseball', '[UNK]', 'safety', 'bench', 'yes', 'yes', '3', 'yes', 'man', 'no', 'yes', 'zebra', '2', 'no', 'no', '[UNK]', 'blue', 'silver', 'yes', 'nowhere', '[UNK]', '4', 'sheep', '[UNK]', '[UNK]', '[UNK]', 'yes', 'canvas', 'tree', 'no', 'yellow', 'no', '4', 'blue', 'yes', 'yellow', 'yes', '[UNK]', 'white', 'trees', 'pizza', '[UNK]', 'leaves', '[UNK]', 'pizza', 'yes', 'yes', 'santa', 'no', '[UNK]', 'yellow', 'no', '4', 'beef', 'many', 'chair', 'wood', 'cows', 'tan', 'man', '0', 'yes', 'plane', 'yes', 'yes', 'yes', 'yes', 'no', 'no', 'white', 'parking', 'yes', '[UNK]', '[UNK]', 'yes', '2', '6', '12', 'blue', 'no', 'sitting', 'yes', 'no', '[UNK]', 'pizza', 'afternoon', '[UNK]', 'wall', 'yes', '1', '2', 'no', 'no', 'right', 'yes', '[UNK]', '2', 'no', '3', '[UNK]', 'zebra', '[UNK]', 'yes', 'yes', '[UNK]', 'red', 'yes', 'white', 'tile', 'red', '9', 'cake', '4', 'gas', 'english', '[UNK]', 'black', 'table', 'no', 'baseball', 'gray', '[UNK]', 'red', 'no', 'yes', '[UNK]', 'nike', 'yes', '5', '3', '2', 'no', '2', 'no', 'yes', 'yes', 'wood', 'no', 'sign', 'no', 'black', '[UNK]', 'baseball', 'yes', '2', 'no', 'athletics', 'yes', 'yes', 'bowl', 'orange', '3', 'yes', 'yes', 'no', 'yes', 'wood', '[UNK]', '[UNK]', 'bathroom', 'no', 'yes', 'tree', 'no', 'no', 'no', 'no', '[UNK]', 'yes', 'sandwich', 'yes', 'resting', 'words', '[UNK]', 'yes', 'yes', 'yes', 'yes', 'yes', 'no', 'camera', '[UNK]', 'sheep', '[UNK]', 'yes', 'no', '4', '[UNK]', 'inside', 'yes', 'watch', 'pine', 'purple', 'no', '3', 'yes', 'nothing', 'no', '6', 'yes', 'couch', 'no', 'yes', 'no', '5', '[UNK]', 'red', 'black', '[UNK]', 'no', '[UNK]', 'kitchen', 'yes', 'railing', 'black', 'pizza', 'no', '4', 'gray', '[UNK]', 'no', 'yellow', 'blue', '[UNK]', 'black', 'net', 'ford', 'yes', '[UNK]', 'yes', 'yes', 'yes', 'no', 'woman', 'no', 'no', 'toilet', 'no', '[UNK]', 'van', 'relaxing', 'umbrella', 'no', 'wedding', 'no', 'black', 'white', 'no', 'no', '[UNK]', 'no', 'yes', 'yes', '2', 'no', '1', 'blue', 'yes', 'yes', 'yes', '2', 'laptop', 'athletics', 'no', '6', 'brown', 'zoo', 'pie', 'yes', 'yes', '2', 'blue', '[UNK]', 'no', 'no', 'no', 'wood', 'black', 'yes', '4', 'no', 'red', 'no', 'no', '2', 'pink', 'black', '[UNK]', 'laptop', '[UNK]', 'yes', '[UNK]', 'building', 'fries', 'city', '[UNK]', '[UNK]', '2', 'food', '[UNK]', 'ground', 'yes', '2', '2', 'bus', 'cows', '3', 'no', 'bathroom', '[UNK]', 'no', 'scissors', 'food', 'yes', 'square', '[UNK]', 'yes', '[UNK]', '[UNK]', '[UNK]', 'leaves', 'brown', '1', 'tennis', 'yes', 'red', '[UNK]', '[UNK]', 'cat', 'no', 'yes', '[UNK]', 'afternoon', 'yes', '2', '7', 'purple', '2', 'yes', 'bathroom', 'no', 'beer', '3', 'no', 'yes', 'blue', '[UNK]', '[UNK]', '[UNK]', 'brown', 'yes', 'black', 'yes', 'bottle', 'no', '[UNK]', 'zebra', '2', 'no', 'black', 'yes', 'brick', 'stop', 'white', 'black', '[UNK]', 'off', 'no', 'silver', 'white', '0', 'yes', '8', 'elephant', 'yes', 'yes', '[UNK]', 'yes', '[UNK]', 'yes', '[UNK]', 'white', '[UNK]', 'no', '1', 'red', 'no', 'friend', '[UNK]', 'no', '0', '[UNK]', 'yes', 'banana', 'pizza', '2', 'no', 'firefighters', 'laughing', 'no', 'yes', '[UNK]', 'yes', 'no', '[UNK]', 'yes', '[UNK]', 'food', 'blue', '[UNK]', 'phones', 'no', 'gray', '[UNK]', 'cowboy', 'white', '2', '20', 'yes', '2', 'tennis', '34', 'vegetables', 'stop', 'night', 'yes', 'surfing', 'many', 'yes', '[UNK]', 'no', '[UNK]', 'yes', 'power', 'yes', '[UNK]', 'purple', 'white', 'yellow', '[UNK]', '[UNK]', 'no', 'elephants', 'yes', 'camera', 'no', 'yes', 'stop', 'brown', 'yes', '1', 'skiing', 'thai', 'green', 'yes', 'fridge', 'yes', 'yes', 'small', '7', 'laptop', 'yes', 'no', '[UNK]', '[UNK]', 'yes', 'yes', 'flat', 'yes', '[UNK]', 'yes', 'yes', 'pink', 'train', 'yellow', 'kitchen', 'yes', '[UNK]', '2', 'pink', 'no', '[UNK]', '[UNK]', 'no', 'zebra', '[UNK]']\n['[UNK]', 'yes', 'thai', 'no', 'yes', 'white', 'water', 'curtains', 'yes', '[UNK]', 'fence', 'yes', '3', 'yes', '[UNK]', '[UNK]', '1', '[UNK]', 'yes', 'umbrella', '[UNK]', 'background', 'cow', 'no', '[UNK]', '[UNK]', 'yes', '[UNK]', 'kitchen', '[UNK]', '1', 'yes', 'yes', 'no', 'blue', 'black', '[UNK]', 'yes', 'yes', 'obsolete', 'no', 'yes', 'no', '5', '[UNK]', '[UNK]', '1', 'yes', 'white', '3', '1', '2', 'bear', 'black', '[UNK]', 'boat', '[UNK]', '2', '3', 'no', 'afternoon', 'yes', '2', 'blue', 'sleeping', 'no', '[UNK]', '[UNK]', 'nature', 'electricity', 'yes', '2', 'yes', 'pyramid', '[UNK]', '9', 'plane', 'tan', '3', 'shirt', '[UNK]', 'white', 'none', 'no', 'white', 'jumping', 'outside', '[UNK]', 'no', 'left', 'no', 'yes', 'green', 'barrel', 'bed', 'posing', 'yes', 'yes', '[UNK]', '3', 'reading', '10', 'no', 'yes', 'snow', '3', 'no', 'black', 'vanilla', 'brown', 'metal', 'cat', 'no', 'no', 'yes', 'yes', '[UNK]', 'red', 'apple', 'yes', 'red', '[UNK]', 'blue', 'no', '[UNK]', 'tile', 'yes', 'vegetables', 'sandwich', '3', 'yes', 'yes', 'yes', 'no', 'tennis', 'brown', 'lamp', 'yes', 'yes', 'yes', 'no', '[UNK]', 'yes', 'yes', 'no', '[UNK]', '2', 'blue', 'steamed', '3', '36', '[UNK]', 'no', '[UNK]', 'red', '[UNK]', 'yes', 'yes', '[UNK]', 'no', 'fruit', '[UNK]', '1', '[UNK]', 'fabric', 'yes', 'no', 'yes', 'tomato', 'no', 'no', 'no', 'grazing', '1', 'plaza', 'chicken', 'tan', 'tennis', 'no', 'yes', '7', 'yes', '[UNK]', '24', 'no', 'computer', 'greyhound', '1', 'no', '1', 'yes', '4', 'red', 'bedroom', 'yes', 'no', 'real', '8', 'no', 'yes', 'no', 'no', 'no', 'rainbow', 'clear', 'white', 'yellow', 'no', '[UNK]', 'yes', 'yes', 'no', 'bathroom', 'yes', 'gray', 'no', 'black', '[UNK]', 'yes', '[UNK]', 'white', 'no', 'yes', 'christmas', 'yes', 'outside', 'baseball', 'yes', 'brown', 'yes', '[UNK]', 'white', 'no', '3', 'skiing', '0', 'yes', 'no', 'yes', 'yellow', 'laptop', 'no', '2', 'yes', 'mountains', 'no', 'pink', 'airplane', 'yes', '[UNK]', 'evening', 'metal', 'no', 'no', 'no', 'truck', '2', '[UNK]', 'yes', 'yes', 'red', '2', '[UNK]', '2', 'japan', 'markers', 'no', '4', 'no', 'standing', 'none', '[UNK]', 'blue', 'mirror', 'yes', '[UNK]', 'yes', 'yes', 'no', '[UNK]', 'swimming', 'left', 'white', '[UNK]', 'catcher', 'blue', 'gray', 'yes', 'black', 'yes', 'no', 'white', 'yes', 'stop', 'blue', '1', 'pizza', 'black', 'no', '[UNK]', 'red', 'no', 'yes', 'england', 'no', 'lot', 'yes', '[UNK]', 'daisy', 'modern', 'no', '1', 'yes', 'black', 'nothing', 'kitchen', '[UNK]', 'blue', 'jumping', '3', 'yes', 'no', '[UNK]', 'steel', 'yes', 'yes', 'bus', '3', 'no', 'handicapped', 'no', 'safety', 'yes', 'blue', '[UNK]', 'no', 'no', '[UNK]', 'black', 'yes', 'sandals', 'yes', 'decoration', '[UNK]', 'yes', 'yes', 'yes', 'purple', 'yes', 'knife', 'no', '4', 'yes', 'no', 'yes', '[UNK]', 'no', 'yes', '[UNK]', 'yes', 'no', 'bed', 'no', 'no', 'yes', 'whale', 'red', '1', 'fruit', 'yes', '[UNK]', 'yes', '4', 'bulls', 'black', 'white', 'log', 'no', 'yes', 'green', 'bathroom', '[UNK]', 'trench', '[UNK]', 'yes', '[UNK]', 'brown', 'yes', 'yes', 'orange', 'yes', 'yes', 'no', 'bird', 'vegetables', 'yes', 'pink', 'field', 'no', 'yes', '11', 'yes', 'purse', 'greyhound', 'nowhere', 'vase', '2', 'refrigerator', '[UNK]', 'trolley', 'mercedes', 'girl', 'tracks', 'yes', 'yes', '[UNK]', '[UNK]', 'red', '40', 'no', 'no', 'bathroom', '[UNK]', '[UNK]', 'no', 'white', 'right', 'yes', 'no', 'no', '[UNK]', 'yes', 'no', 'no', 'yes', '1', '1', 'no', 'yes', '[UNK]', '[UNK]', '[UNK]', 'yes', 'no', 'samsung', '[UNK]', 'luggage', '2', 'yes', 'no', 'laptop', 'green', 'yes', 'yes', 'no', '[UNK]', '3', 'yes', 'yes', '3', 'yes', 'chair', '[UNK]', '[UNK]', '1', 'grapes', '1', 'no', 'street', 'no', 'fishing', 'yes', 'yes', 'none', 'blue', '[UNK]', 'ukraine', '1', 'box', '[UNK]', 'yes', 'no', 'gray', '3', 'yes', '[UNK]', 'no', '6', '[UNK]', 'ball', 'no', 'electricity', 'yes', 'yes', 'walking', 'rose', '[UNK]', 'runway', '1', 'white', 'yellow', 'no', 'yes', 'table', 'snow', '0', 'no', 'no', '[UNK]', '[UNK]', '1', 'yes', 'no', 'no', 'american', '[UNK]', 'baseball', '2', 'yes', 'yes', '4', 'yes', 'train', 'yes', 'yes', 'afternoon', '1', 'yes', 'no', '[UNK]', '[UNK]', 'white', 'no', 'south', 'black', '45', 'american', 'none', 'blue', '[UNK]', 'yes', 'sandwich', 'log', 'yes', '[UNK]', 'yes', '3', 'white', 'no', 'orange', 'yes', 'brown', 'white', 'people', '[UNK]', 'board', 'leaves', '[UNK]', 'women', 'yes', 'yes', '[UNK]', 'yes', '[UNK]', '[UNK]', 'no', '5', 'tomato', '5', 'toilet', '[UNK]', 'ox', 'white', 'women', '0', 'no', 'airplane', 'no', 'yes', 'yes', 'yes', 'no', 'no', 'brown', 'tree', 'yes', '[UNK]', '[UNK]', 'no', '1', 'electricity', '2', '[UNK]', 'no', 'sitting', 'no', 'yes', '[UNK]', 'bread', 'night', 'fork', 'abstract', 'yes', '1', '1', 'no', 'no', 'forward', 'no', 'yellow', '1', 'yes', '2', 'banana', 'field', '[UNK]', 'no', 'yes', 'table', 'truck', 'yes', 'green', '[UNK]', 'white', '9', 'cake', '4', 'pizza', 'japanese', 'potatoes', 'black', 'left', 'yes', 'bat', 'brown', '[UNK]', 'curry', 'no', 'yes', 'shirt', 'nike', 'no', '25', '[UNK]', '1', 'no', '1', 'yes', 'no', 'no', 'wood', 'no', '[UNK]', 'yes', 'black', 'green', 'bats', 'no', '1', 'yes', 'batter', 'yes', 'no', '[UNK]', 'tomato', '0', 'yes', 'yes', 'yes', 'no', 'wood', 'plate', '[UNK]', 'bathroom', 'yes', 'no', 'cat', 'no', 'no', 'no', 'no', 'beer', 'yes', 'cake', 'no', 'standing', 'md', 'nbc', 'yes', 'yes', 'yes', 'yes', 'yes', 'yes', 'boy', 'usa', 'sheep', '[UNK]', 'yes', 'no', '1', '[UNK]', 'inside', 'yes', 'glove', 'pine', 'candle', 'no', '0', 'no', 'ball', 'no', '6', 'yes', 'laptop', 'yes', 'yes', 'no', '10', '[UNK]', '[UNK]', 'white', 'far', 'no', 'short', '[UNK]', 'yes', 'table', 'black', '[UNK]', 'yes', '5', 'english', '[UNK]', 'yes', 'orange', 'blue', '[UNK]', 'black', 'standing', '[UNK]', 'no', 'shadows', 'no', 'no', 'yes', 'no', 'people', 'yes', 'no', 'tank', 'no', 'pay', 'people', 'reflection', 'boxes', 'no', 'cake', 'no', 'gray', 'gray', 'no', 'yes', 'teddy', 'no', 'yes', 'no', 'very', 'no', '3', 'blue', 'no', 'no', 'no', '1', '[UNK]', 'athletics', 'yes', '6', '[UNK]', 'zoo', '[UNK]', 'no', 'no', '1', 'blue', 'wall', 'yes', 'yes', 'yes', 'wood', 'red', 'yes', '8', 'no', 'red', 'yes', 'yes', '2', 'pink', 'black', 'wood', 'cane', '[UNK]', 'yes', '[UNK]', '[UNK]', 'restaurant', '[UNK]', '[UNK]', '[UNK]', '1', '[UNK]', '[UNK]', 'grass', 'yes', '1', '3', 'bus', 'hay', '3', 'no', 'cabinets', 'peanut', 'no', 'eggs', 'kitchen', 'yes', 'square', 'gray', 'no', 'supreme', '[UNK]', 'christmas', 'leaves', 'brown', '1', 'tennis', 'no', 'red', 'field', 'names', 'cat', 'no', 'yes', 'vacuum', 'daytime', 'yes', '0', '6', '2', '1', 'no', 'bathroom', 'yes', 'beer', '3', 'yes', 'no', '[UNK]', '[UNK]', 'peace', 'real', '[UNK]', 'yes', 'black', 'yes', 'couch', 'yes', '[UNK]', 'zebra', '3', 'no', '[UNK]', 'no', 'pharmacy', 'stop', 'black', 'black', '[UNK]', 'on', 'no', 'white', 'white', '1', 'no', '33', 'bird', 'yes', 'no', 'logos', 'no', 'hooks', 'no', 'outside', 'blue', '[UNK]', 'yes', 'p', 'blue', 'yes', '[UNK]', 'bicycle', 'no', '15', 'nothing', 'yes', '[UNK]', 'fork', '1', 'no', 'water', '[UNK]', 'yes', 'no', '[UNK]', 'yes', 'yes', '[UNK]', 'yes', 'noon', '[UNK]', 'blue', 'left', 'samsung', 'no', 'black', '[UNK]', '[UNK]', 'red', '3', '4', 'no', '2', '[UNK]', '[UNK]', 'cake', '[UNK]', '[UNK]', 'no', 'surfing', '50', 'no', '[UNK]', 'no', 'umbrella', 'yes', '[UNK]', 'no', '[UNK]', 'pink', 'black', 'orange', 'brown', '[UNK]', 'no', 'elephants', '2', 'ball', 'yes', 'yes', '[UNK]', 'black', 'urban', '2', 'skiing', 'thai', 'blue', 'yes', 'full', 'yes', 'no', '[UNK]', '1', 'lamp', 'yes', 'yes', '[UNK]', 'turkey', 'no', 'yes', 'barrel', 'yes', 'baseball', 'yes', 'no', 'brown', 'passenger', 'yellow', 'office', 'yes', 'in', '3', 'green', 'no', 'home', '[UNK]', 'yes', '[UNK]', 'turkey']\nAccuracy: 0.4120\nMicro Precision: 0.4120\nMicro Recall: 0.4120\nMicro F1 Score: 0.4120\nMacro Precision: 0.4120\nMacro Recall: 0.4120\nMacro F1 Score: 0.4120\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\n\n# Define the path to save the model\nsave_path = \"/kaggle/working/vqa_model.pth\"\n\n# Save the model\ntorch.save(model.state_dict(), save_path)\n\n# Check if the file is saved\nif os.path.exists(save_path):\n    print(\"Model saved successfully at:\", save_path)\nelse:\n    print(\"Failed to save the model.\")\n","metadata":{"execution":{"iopub.status.busy":"2024-05-18T11:46:10.746046Z","iopub.execute_input":"2024-05-18T11:46:10.746949Z","iopub.status.idle":"2024-05-18T11:46:12.172999Z","shell.execute_reply.started":"2024-05-18T11:46:10.746915Z","shell.execute_reply":"2024-05-18T11:46:12.172076Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Model saved successfully at: /kaggle/working/vqa_model.pth\n","output_type":"stream"}]},{"cell_type":"code","source":"import pickle\n\n# Define the path to save the model\nsave_path = \"/kaggle/working/vqa_model.pkl\"\n\n# Save the model\nwith open(save_path, 'wb') as f:\n    pickle.dump(model.state_dict(), f)\n\n    \n# Check if the file is saved\nif os.path.exists(save_path):\n    print(\"Model saved successfully at:\", save_path)\nelse:\n    print(\"Failed to save the model.\")\n","metadata":{"execution":{"iopub.status.busy":"2024-05-18T11:46:18.971929Z","iopub.execute_input":"2024-05-18T11:46:18.972827Z","iopub.status.idle":"2024-05-18T11:46:20.555993Z","shell.execute_reply.started":"2024-05-18T11:46:18.972787Z","shell.execute_reply":"2024-05-18T11:46:20.555028Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Model saved successfully at: /kaggle/working/vqa_model.pkl\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}